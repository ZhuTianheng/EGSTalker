# EGSTalker
Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation

---

## ğŸ§  Introduction

**EGSTalker** is a real-time audio-driven talking head generation framework based on 3D Gaussian deformation.  
We propose an efficient spatial-audio attention (ESAA) mechanism and Kolmogorov-Arnold Network (KAN) based deformation decoder to achieve high-fidelity and synchronized talking head synthesis with significant inference speed improvements.

This repository contains the official implementation of the paper:

> **EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation**  
> [Author] Tianheng Zhu, Yinfeng Yu*, Liejun Wang, Fuchun Sun, Wendong Zheng  
> (*Corresponding author)

## ğŸ“œ Paper

> [Coming soon]

## ğŸ“½ï¸ Demo

### Audio-Driven Talking Head Synthesis
ğŸ‘‰ [Download the demo video (EGSTalker.mp4)](https://github.com/ZhuTianheng/EGSTalker/raw/main/result-video/EGSTalker.mp4)
<!--
## ğŸ“¦ Installation

We recommend using **Conda** to set up the environment.

1. Clone the repository:
    ```bash
    git clone https://github.com/ZhuTianheng/EGSTalker.git
    cd EGSTalker
    ```

2. Create the Conda environment from the provided YAML file:
    ```bash
    conda env create -f environment.yml
    ```

3. Activate the environment:
    ```bash
    conda activate egstalker
    ```

4. (Optional) Install external dependencies (e.g., CUDA-based Gaussian rasterization modules).

## ğŸ› ï¸ Usage

To train the model:

```bash
python train.py --config configs/your_config.yaml
-->
